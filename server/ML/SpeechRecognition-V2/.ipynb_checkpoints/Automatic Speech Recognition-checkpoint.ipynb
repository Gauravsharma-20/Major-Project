{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d95edf4",
   "metadata": {},
   "source": [
    "# Importing all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "526b4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import json\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import librosa\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "RNG_SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d8931",
   "metadata": {},
   "source": [
    "# Mapping Integer to Character Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69ff1a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Map -  {1: \"'\", 2: ' ', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z'}\n",
      "****************************************************************************************************\n",
      "Char Map -  {\"'\": 0, '<SPACE>': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n"
     ]
    }
   ],
   "source": [
    "# Map for converting integer and text sequences\n",
    "char_map_str = \"\"\"\n",
    "' 0\n",
    "<SPACE> 1\n",
    "a 2\n",
    "b 3\n",
    "c 4\n",
    "d 5\n",
    "e 6\n",
    "f 7\n",
    "g 8\n",
    "h 9\n",
    "i 10\n",
    "j 11\n",
    "k 12\n",
    "l 13\n",
    "m 14\n",
    "n 15\n",
    "o 16\n",
    "p 17\n",
    "q 18\n",
    "r 19\n",
    "s 20\n",
    "t 21\n",
    "u 22\n",
    "v 23\n",
    "w 24\n",
    "x 25\n",
    "y 26\n",
    "z 27\n",
    "\"\"\"\n",
    "# the \"blank\" character is mapped to 28\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "for line in char_map_str.strip().split('\\n'):\n",
    "    ch, index = line.split()\n",
    "    char_map[ch] = int(index)\n",
    "    index_map[int(index)+1] = ch\n",
    "index_map[2] = ' '\n",
    "\n",
    "print(\"Index Map - \", index_map)\n",
    "print('*'*100)\n",
    "print(\"Char Map - \", char_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701a812",
   "metadata": {},
   "source": [
    "# Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f5b058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_int_sequence(text):\n",
    "    int_sequence = []\n",
    "    for c in text:\n",
    "        if c == ' ':\n",
    "            ch = char_map['<SPACE>']\n",
    "        else:\n",
    "            ch = char_map[c]\n",
    "        int_sequence.append(ch)\n",
    "    \n",
    "    return int_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e14eefd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text_sequence(int_sequence):\n",
    "    text = []\n",
    "    for i in int_sequence:\n",
    "        text.append(index_map[i])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cbac0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feat_dim(window, max_freq):\n",
    "    return int(0.001 * window * max_freq) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed0ccb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
    "    \"\"\"\n",
    "    Compute the spectrogram for a real signal.\n",
    "    The parameters follow the naming convention of\n",
    "    matplotlib.mlab.specgram\n",
    "\n",
    "    Args:\n",
    "        samples (1D array): input audio signal\n",
    "        fft_length (int): number of elements in fft window\n",
    "        sample_rate (scalar): sample rate\n",
    "        hop_length (int): hop length (relative offset between neighboring\n",
    "            fft windows).\n",
    "\n",
    "    Returns:\n",
    "        x (2D array): spectrogram [frequency x time]\n",
    "        freq (1D array): frequency of each row in x\n",
    "\n",
    "    Note:\n",
    "        This is a truncating computation e.g. if fft_length=10,\n",
    "        hop_length=5 and the signal has 23 elements, then the\n",
    "        last 3 elements will be truncated.\n",
    "    \"\"\"\n",
    "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
    "\n",
    "    window = np.hanning(fft_length)[:, None]\n",
    "    window_norm = np.sum(window**2)\n",
    "\n",
    "    # The scaling below follows the convention of\n",
    "    # matplotlib.mlab.specgram which is the same as\n",
    "    # matlabs specgram.\n",
    "    scale = window_norm * sample_rate\n",
    "\n",
    "    trunc = (len(samples) - fft_length) % hop_length\n",
    "    x = samples[:len(samples) - trunc]\n",
    "\n",
    "    # \"stride trick\" reshape to include overlap\n",
    "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
    "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
    "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
    "\n",
    "    # window stride sanity check\n",
    "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
    "\n",
    "    # broadcast window, compute fft over columns and square mod\n",
    "    x = np.fft.rfft(x * window, axis=0)\n",
    "    x = np.absolute(x)**2\n",
    "\n",
    "    # scale, 2.0 for everything except dc and fft_length/2\n",
    "    x[1:-1, :] *= (2.0 / scale)\n",
    "    x[(0, -1), :] /= scale\n",
    "\n",
    "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
    "\n",
    "    return x, freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2942449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectogram_from_file(filename, step=10, window=20, max_freq=None, eps=1e-14):\n",
    "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
    "    Params:\n",
    "        filename (str): Path to the audio file\n",
    "        step (int): Step size in milliseconds between windows\n",
    "        window (int): FFT window size in milliseconds\n",
    "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "            [0, max_freq] are returned\n",
    "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    with soundfile.SoundFile(filename) as sound_file:\n",
    "        audio = sound_file.read(dtype='float32')\n",
    "        sample_rate = sound_file.samplerate\n",
    "        \n",
    "        if audio.ndim >= 2:\n",
    "            audio = np.mean(audio, 1)\n",
    "        if max_freq is None:\n",
    "            max_freq = sample_rate / 2\n",
    "        if max_freq > sample_rate / 2:\n",
    "            raise ValueError(\"max_freq must not be greater than half of sample rate\")\n",
    "        \n",
    "        if step > window:\n",
    "            raise ValueError(\"step size can not be greater than window size\")\n",
    "            \n",
    "        hop_length = int(0.001 * step * sample_rate)\n",
    "        fft_length = int(0.001 * window * sample_rate)\n",
    "        \n",
    "        pxx, freqs = spectrogram(audio, fft_length=fft_length, sample_rate=sample_rate, hop_length=hop_length)\n",
    "        ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "        \n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22726e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-32.2361913 , -32.2361913 , -32.2361913 , ..., -32.2361913 ,\n",
       "        -32.2361913 , -32.2361913 ],\n",
       "       [-32.2361913 , -32.2361913 , -32.2361913 , ..., -32.2361913 ,\n",
       "        -32.2361913 , -32.2361913 ],\n",
       "       [-32.2361913 , -32.2361913 , -32.2361913 , ..., -32.2361913 ,\n",
       "        -32.2361913 , -32.2361913 ],\n",
       "       ...,\n",
       "       [-19.46345241, -16.50608503, -14.75336718, ..., -31.91459109,\n",
       "        -32.00709703, -32.06067494],\n",
       "       [-19.62046312, -16.42074475, -15.59201998, ..., -32.23595013,\n",
       "        -32.17456839, -32.22239232],\n",
       "       [-19.91432965, -17.27890566, -15.5476278 , ..., -31.90016734,\n",
       "        -32.20705714, -32.18862531]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectogram_from_file_old('./FinalTestFiles/Trump New.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf1115",
   "metadata": {},
   "source": [
    "# Utility classes and functions for featurizing audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30194fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGenerator():\n",
    "    def __init__(self, step=10, window=20, max_freq=8000, mfcc_dim=13,\n",
    "        minibatch_size=20, desc_file=None, spectrogram=True, max_duration=10.0, \n",
    "        sort_by_duration=False):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            step (int): Step size in milliseconds between windows (for spectrogram ONLY)\n",
    "            window (int): FFT window size in milliseconds (for spectrogram ONLY)\n",
    "            max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "                [0, max_freq] are returned (for spectrogram ONLY)\n",
    "            desc_file (str, optional): Path to a JSON-line file that contains\n",
    "                labels and paths to the audio files. If this is None, then\n",
    "                load metadata right away\n",
    "        \"\"\"\n",
    "\n",
    "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
    "        self.mfcc_dim = mfcc_dim\n",
    "        self.feats_mean = np.zeros((self.feat_dim,))\n",
    "        self.feats_std = np.ones((self.feat_dim,))\n",
    "        self.rng = random.Random(RNG_SEED)\n",
    "        if desc_file is not None:\n",
    "            self.load_metadata_from_desc_file(desc_file)\n",
    "        self.step = step\n",
    "        self.window = window\n",
    "        self.max_freq = max_freq\n",
    "        self.cur_train_index = 0\n",
    "        self.cur_valid_index = 0\n",
    "        self.cur_test_index = 0\n",
    "        self.max_duration=max_duration\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.spectrogram = spectrogram\n",
    "        self.sort_by_duration = sort_by_duration\n",
    "\n",
    "    def get_batch(self, partition):\n",
    "        \"\"\" Obtain a batch of train, validation, or test data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            audio_paths = self.train_audio_paths\n",
    "            cur_index = self.cur_train_index\n",
    "            texts = self.train_texts\n",
    "        elif partition == 'valid':\n",
    "            audio_paths = self.valid_audio_paths\n",
    "            cur_index = self.cur_valid_index\n",
    "            texts = self.valid_texts\n",
    "        elif partition == 'test':\n",
    "            audio_paths = self.test_audio_paths\n",
    "            cur_index = self.test_valid_index\n",
    "            texts = self.test_texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "        features = [self.normalize(self.featurize(a)) for a in \n",
    "            audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
    "\n",
    "        # calculate necessary sizes\n",
    "        max_length = max([features[i].shape[0] \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        max_string_length = max([len(texts[cur_index+i]) \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        \n",
    "        # initialize the arrays\n",
    "        X_data = np.zeros([self.minibatch_size, max_length, \n",
    "            self.feat_dim*self.spectrogram + self.mfcc_dim*(not self.spectrogram)])\n",
    "        labels = np.ones([self.minibatch_size, max_string_length]) * 28\n",
    "        input_length = np.zeros([self.minibatch_size, 1])\n",
    "        label_length = np.zeros([self.minibatch_size, 1])\n",
    "        \n",
    "        for i in range(0, self.minibatch_size):\n",
    "            # calculate X_data & input_length\n",
    "            feat = features[i]\n",
    "            input_length[i] = feat.shape[0]\n",
    "            X_data[i, :feat.shape[0], :] = feat\n",
    "\n",
    "            # calculate labels & label_length\n",
    "            label = np.array(text_to_int_sequence(texts[cur_index+i])) \n",
    "            labels[i, :len(label)] = label\n",
    "            label_length[i] = len(label)\n",
    " \n",
    "        # return the arrays\n",
    "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
    "        inputs = {'the_input': X_data, \n",
    "                  'the_labels': labels, \n",
    "                  'input_length': input_length, \n",
    "                  'label_length': label_length \n",
    "                 }\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    def shuffle_data_by_partition(self, partition):\n",
    "        \"\"\" Shuffle the training or validation data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = shuffle_data(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = shuffle_data(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def sort_data_by_duration(self, partition):\n",
    "        \"\"\" Sort the training or validation sets by (increasing) duration\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = sort_data(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = sort_data(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def next_train(self):\n",
    "        \"\"\" Obtain a batch of training data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('train')\n",
    "            self.cur_train_index += self.minibatch_size\n",
    "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
    "                self.cur_train_index = 0\n",
    "                self.shuffle_data_by_partition('train')\n",
    "            yield ret    \n",
    "\n",
    "    def next_valid(self):\n",
    "        \"\"\" Obtain a batch of validation data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('valid')\n",
    "            self.cur_valid_index += self.minibatch_size\n",
    "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
    "                self.cur_valid_index = 0\n",
    "                self.shuffle_data_by_partition('valid')\n",
    "            yield ret\n",
    "\n",
    "    def next_test(self):\n",
    "        \"\"\" Obtain a batch of test data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('test')\n",
    "            self.cur_test_index += self.minibatch_size\n",
    "            if self.cur_test_index >= len(self.test_texts) - self.minibatch_size:\n",
    "                self.cur_test_index = 0\n",
    "            yield ret\n",
    "\n",
    "    def load_train_data(self, desc_file='train_corpus.json'):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'train')\n",
    "        self.fit_train()\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('train')\n",
    "\n",
    "    def load_validation_data(self, desc_file='valid_corpus.json'):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'validation')\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('valid')\n",
    "\n",
    "    def load_test_data(self, desc_file='test_corpus.json'):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'test')\n",
    "    \n",
    "    def load_metadata_from_desc_file(self, desc_file, partition):\n",
    "        \"\"\" Read metadata from a JSON-line file\n",
    "            (possibly takes long, depending on the filesize)\n",
    "        Params:\n",
    "            desc_file (str):  Path to a JSON-line file that contains labels and\n",
    "                paths to the audio files\n",
    "            partition (str): One of 'train', 'validation' or 'test'\n",
    "        \"\"\"\n",
    "        audio_paths, durations, texts = [], [], []\n",
    "        with open(desc_file) as json_line_file:\n",
    "            for line_num, json_line in enumerate(json_line_file):\n",
    "                try:\n",
    "                    spec = json.loads(json_line)\n",
    "                    if float(spec['duration']) > self.max_duration:\n",
    "                        continue\n",
    "                    audio_paths.append(spec['key'])\n",
    "                    durations.append(float(spec['duration']))\n",
    "                    texts.append(spec['text'])\n",
    "                except Exception as e:\n",
    "                    # Change to (KeyError, ValueError) or\n",
    "                    # (KeyError,json.decoder.JSONDecodeError), depending on\n",
    "                    # json module version\n",
    "                    print('Error reading line #{}: {}'\n",
    "                                .format(line_num, json_line))\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths = audio_paths\n",
    "            self.train_durations = durations\n",
    "            self.train_texts = texts\n",
    "        elif partition == 'validation':\n",
    "            self.valid_audio_paths = audio_paths\n",
    "            self.valid_durations = durations\n",
    "            self.valid_texts = texts\n",
    "        elif partition == 'test':\n",
    "            self.test_audio_paths = audio_paths\n",
    "            self.test_durations = durations\n",
    "            self.test_texts = texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition to load metadata. \"\n",
    "             \"Must be train/validation/test\")\n",
    "            \n",
    "    def fit_train(self, k_samples=100):\n",
    "        \"\"\" Estimate the mean and std of the features from the training set\n",
    "        Params:\n",
    "            k_samples (int): Use this number of samples for estimation\n",
    "        \"\"\"\n",
    "        k_samples = min(k_samples, len(self.train_audio_paths))\n",
    "        samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
    "        feats = [self.featurize(s) for s in samples]\n",
    "        feats = np.vstack(feats)\n",
    "        self.feats_mean = np.mean(feats, axis=0)\n",
    "        self.feats_std = np.std(feats, axis=0)\n",
    "        \n",
    "    def featurize(self, audio_clip):\n",
    "        \"\"\" For a given audio clip, calculate the corresponding feature\n",
    "        Params:\n",
    "            audio_clip (str): Path to the audio clip\n",
    "        \"\"\"\n",
    "        if self.spectrogram:\n",
    "            return spectrogram_from_file(\n",
    "                audio_clip, step=self.step, window=self.window,\n",
    "                max_freq=self.max_freq)\n",
    "        else:\n",
    "            (rate, sig) = wav.read(audio_clip)\n",
    "            return mfcc(sig, rate, numcep=self.mfcc_dim)\n",
    "\n",
    "    def normalize(self, feature, eps=1e-14):\n",
    "        \"\"\" Center a feature using the mean and std\n",
    "        Params:\n",
    "            feature (numpy.ndarray): Feature to normalize\n",
    "        \"\"\"\n",
    "        return (feature - self.feats_mean) / (self.feats_std + eps)\n",
    "\n",
    "def shuffle_data(audio_paths, durations, texts):\n",
    "    \"\"\" Shuffle the data (called after making a complete pass through \n",
    "        training or validation data during the training process)\n",
    "    Params:\n",
    "        audio_paths (list): Paths to audio clips\n",
    "        durations (list): Durations of utterances for each audio clip\n",
    "        texts (list): Sentences uttered in each audio clip\n",
    "    \"\"\"\n",
    "    p = np.random.permutation(len(audio_paths))\n",
    "    audio_paths = [audio_paths[i] for i in p] \n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "def sort_data(audio_paths, durations, texts):\n",
    "    \"\"\" Sort the data by duration \n",
    "    Params:\n",
    "        audio_paths (list): Paths to audio clips\n",
    "        durations (list): Durations of utterances for each audio clip\n",
    "        texts (list): Sentences uttered in each audio clip\n",
    "    \"\"\"\n",
    "    p = np.argsort(durations).tolist()\n",
    "    audio_paths = [audio_paths[i] for i in p]\n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "def vis_train_features(index=0):\n",
    "    \"\"\" Visualizing the data point in the training set at the supplied index\n",
    "    \"\"\"\n",
    "    # obtain spectrogram\n",
    "    audio_gen = AudioGenerator(spectrogram=True)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_audio_path = audio_gen.train_audio_paths[index]\n",
    "    vis_spectrogram_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # obtain mfcc\n",
    "    audio_gen = AudioGenerator(spectrogram=False)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_mfcc_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # obtain text label\n",
    "    vis_text = audio_gen.train_texts[index]\n",
    "    # obtain raw audio\n",
    "    vis_raw_audio, _ = librosa.load(vis_audio_path)\n",
    "    # print total number of training examples\n",
    "    print('There are %d total training examples.' % len(audio_gen.train_audio_paths))\n",
    "    # return labels for plotting\n",
    "    return vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path\n",
    "\n",
    "\n",
    "def plot_raw_audio(vis_raw_audio):\n",
    "    # plot the raw audio signal\n",
    "    fig = plt.figure(figsize=(12,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    steps = len(vis_raw_audio)\n",
    "    ax.plot(np.linspace(1, steps, steps), vis_raw_audio)\n",
    "    plt.title('Audio Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "def plot_mfcc_feature(vis_mfcc_feature):\n",
    "    # plot the MFCC feature\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized MFCC')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('MFCC Coefficient')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectrogram_feature(vis_spectrogram_feature):\n",
    "    # plot the normalized spectrogram\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_spectrogram_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized Spectrogram')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('Frequency')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199fdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5036cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
