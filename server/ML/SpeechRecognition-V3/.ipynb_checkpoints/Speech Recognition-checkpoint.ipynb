{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf95e1dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import json\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import librosa\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "import _pickle as pickle\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping   \n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (Input, Lambda, BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM, Embedding)\n",
    "\n",
    "RNG_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3483445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_int_sequence(text):\n",
    "    int_sequence = []\n",
    "    for c in text:\n",
    "        if c == ' ':\n",
    "            ch = char_map['<SPACE>']\n",
    "        else:\n",
    "            ch = char_map[c]\n",
    "        int_sequence.append(ch)\n",
    "    \n",
    "    return int_sequence\n",
    "\n",
    "def int_sequence_to_text(int_sequence):\n",
    "    text = []\n",
    "    for i in int_sequence:\n",
    "        text.append(index_map[i])\n",
    "    return text\n",
    "\n",
    "def calc_feat_dim(window, max_freq):\n",
    "    return int(0.001 * window * max_freq) + 1\n",
    "\n",
    "def conv_output_length(input_length, filter_size, border_mode, stride,\n",
    "                       dilation=1):\n",
    "    \"\"\" Compute the length of the output sequence after 1D convolution along\n",
    "        time. Note that this function is in line with the function used in\n",
    "        Convolution1D class from Keras.\n",
    "    Params:\n",
    "        input_length (int): Length of the input sequence.\n",
    "        filter_size (int): Width of the convolution kernel.\n",
    "        border_mode (str): Only support `same` or `valid`.\n",
    "        stride (int): Stride size used in 1D convolution.\n",
    "        dilation (int)\n",
    "    \"\"\"\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    assert border_mode in {'same', 'valid'}\n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "    if border_mode == 'same':\n",
    "        output_length = input_length\n",
    "    elif border_mode == 'valid':\n",
    "        output_length = input_length - dilated_filter_size + 1\n",
    "    return (output_length + stride - 1) // stride\n",
    "\n",
    "\n",
    "def spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
    "    \"\"\"\n",
    "    Compute the spectrogram for a real signal.\n",
    "    The parameters follow the naming convention of\n",
    "    matplotlib.mlab.specgram\n",
    "\n",
    "    Args:\n",
    "        samples (1D array): input audio signal\n",
    "        fft_length (int): number of elements in fft window\n",
    "        sample_rate (scalar): sample rate\n",
    "        hop_length (int): hop length (relative offset between neighboring\n",
    "            fft windows).\n",
    "\n",
    "    Returns:\n",
    "        x (2D array): spectrogram [frequency x time]\n",
    "        freq (1D array): frequency of each row in x\n",
    "\n",
    "    Note:\n",
    "        This is a truncating computation e.g. if fft_length=10,\n",
    "        hop_length=5 and the signal has 23 elements, then the\n",
    "        last 3 elements will be truncated.\n",
    "    \"\"\"\n",
    "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
    "\n",
    "    window = np.hanning(fft_length)[:, None]\n",
    "    window_norm = np.sum(window**2)\n",
    "\n",
    "    # The scaling below follows the convention of\n",
    "    # matplotlib.mlab.specgram which is the same as\n",
    "    # matlabs specgram.\n",
    "    scale = window_norm * sample_rate\n",
    "\n",
    "    trunc = (len(samples) - fft_length) % hop_length\n",
    "    x = samples[:len(samples) - trunc]\n",
    "\n",
    "    # \"stride trick\" reshape to include overlap\n",
    "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
    "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
    "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
    "\n",
    "    # window stride sanity check\n",
    "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
    "\n",
    "    # broadcast window, compute fft over columns and square mod\n",
    "    x = np.fft.rfft(x * window, axis=0)\n",
    "    x = np.absolute(x)**2\n",
    "\n",
    "    # scale, 2.0 for everything except dc and fft_length/2\n",
    "    x[1:-1, :] *= (2.0 / scale)\n",
    "    x[(0, -1), :] /= scale\n",
    "\n",
    "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
    "\n",
    "    return x, freqs\n",
    "\n",
    "\n",
    "def spectrogram_from_file(filename, step=10, window=20, max_freq=None, eps=1e-14):\n",
    "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
    "    Params:\n",
    "        filename (str): Path to the audio file\n",
    "        step (int): Step size in milliseconds between windows\n",
    "        window (int): FFT window size in milliseconds\n",
    "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "            [0, max_freq] are returned\n",
    "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    with soundfile.SoundFile(filename) as sound_file:\n",
    "        audio = sound_file.read(dtype='float32')\n",
    "        sample_rate = sound_file.samplerate\n",
    "        \n",
    "        if audio.ndim >= 2:\n",
    "            audio = np.mean(audio, 1)\n",
    "        if max_freq is None:\n",
    "            max_freq = sample_rate / 2\n",
    "        if max_freq > sample_rate / 2:\n",
    "            raise ValueError(\"max_freq must not be greater than half of sample rate\")\n",
    "        \n",
    "        if step > window:\n",
    "            raise ValueError(\"step size can not be greater than window size\")\n",
    "            \n",
    "        hop_length = int(0.001 * step * sample_rate)\n",
    "        fft_length = int(0.001 * window * sample_rate)\n",
    "        \n",
    "        pxx, freqs = spectrogram(audio, fft_length=fft_length, sample_rate=sample_rate, hop_length=hop_length)\n",
    "        ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "        \n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b2741ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGenerator():\n",
    "    def __init__(self, step=10, window=20, max_freq=8000, mfcc_dim=13,\n",
    "        minibatch_size=20, desc_file=None, spectrogram=True, max_duration=10.0, \n",
    "        sort_by_duration=False):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            step (int): Step size in milliseconds between windows (for spectrogram ONLY)\n",
    "            window (int): FFT window size in milliseconds (for spectrogram ONLY)\n",
    "            max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "                [0, max_freq] are returned (for spectrogram ONLY)\n",
    "            desc_file (str, optional): Path to a JSON-line file that contains\n",
    "                labels and paths to the audio files. If this is None, then\n",
    "                load metadata right away\n",
    "        \"\"\"\n",
    "\n",
    "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
    "        self.mfcc_dim = mfcc_dim\n",
    "        self.feats_mean = np.zeros((self.feat_dim,))\n",
    "        self.feats_std = np.ones((self.feat_dim,))\n",
    "        self.rng = random.Random(RNG_SEED)\n",
    "        if desc_file is not None:\n",
    "            self.load_metadata_from_desc_file(desc_file)\n",
    "        self.step = step\n",
    "        self.window = window\n",
    "        self.max_freq = max_freq\n",
    "        self.cur_train_index = 0\n",
    "        self.cur_valid_index = 0\n",
    "        self.cur_test_index = 0\n",
    "        self.max_duration=max_duration\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.spectrogram = spectrogram\n",
    "        self.sort_by_duration = sort_by_duration\n",
    "\n",
    "    def get_batch(self, partition):\n",
    "        \"\"\" Obtain a batch of train, validation, or test data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            audio_paths = self.train_audio_paths\n",
    "            cur_index = self.cur_train_index\n",
    "            texts = self.train_texts\n",
    "        elif partition == 'valid':\n",
    "            audio_paths = self.valid_audio_paths\n",
    "            cur_index = self.cur_valid_index\n",
    "            texts = self.valid_texts\n",
    "        elif partition == 'test':\n",
    "            audio_paths = self.test_audio_paths\n",
    "            cur_index = self.test_valid_index\n",
    "            texts = self.test_texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "        features = [self.normalize(self.featurize(a)) for a in \n",
    "            audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
    "\n",
    "        # calculate necessary sizes\n",
    "        max_length = max([features[i].shape[0] \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        max_string_length = max([len(texts[cur_index+i]) \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        \n",
    "        # initialize the arrays\n",
    "        X_data = np.zeros([self.minibatch_size, max_length, \n",
    "            self.feat_dim*self.spectrogram + self.mfcc_dim*(not self.spectrogram)])\n",
    "        labels = np.ones([self.minibatch_size, max_string_length]) * 28\n",
    "        input_length = np.zeros([self.minibatch_size, 1])\n",
    "        label_length = np.zeros([self.minibatch_size, 1])\n",
    "        \n",
    "        for i in range(0, self.minibatch_size):\n",
    "            # calculate X_data & input_length\n",
    "            feat = features[i]\n",
    "            input_length[i] = feat.shape[0]\n",
    "            X_data[i, :feat.shape[0], :] = feat\n",
    "\n",
    "            # calculate labels & label_length\n",
    "            label = np.array(text_to_int_sequence(texts[cur_index+i])) \n",
    "            labels[i, :len(label)] = label\n",
    "            label_length[i] = len(label)\n",
    " \n",
    "        # return the arrays\n",
    "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
    "        inputs = {'the_input': X_data, \n",
    "                  'the_labels': labels, \n",
    "                  'input_length': input_length, \n",
    "                  'label_length': label_length \n",
    "                 }\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    def shuffle_data_by_partition(self, partition):\n",
    "        \"\"\" Shuffle the training or validation data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = shuffle_data(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = shuffle_data(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def sort_data_by_duration(self, partition):\n",
    "        \"\"\" Sort the training or validation sets by (increasing) duration\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = sort_data(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = sort_data(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def next_train(self):\n",
    "        \"\"\" Obtain a batch of training data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('train')\n",
    "            self.cur_train_index += self.minibatch_size\n",
    "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
    "                self.cur_train_index = 0\n",
    "                self.shuffle_data_by_partition('train')\n",
    "            yield ret    \n",
    "\n",
    "    def next_valid(self):\n",
    "        \"\"\" Obtain a batch of validation data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('valid')\n",
    "            self.cur_valid_index += self.minibatch_size\n",
    "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
    "                self.cur_valid_index = 0\n",
    "                self.shuffle_data_by_partition('valid')\n",
    "            yield ret\n",
    "\n",
    "    def next_test(self):\n",
    "        \"\"\" Obtain a batch of test data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('test')\n",
    "            self.cur_test_index += self.minibatch_size\n",
    "            if self.cur_test_index >= len(self.test_texts) - self.minibatch_size:\n",
    "                self.cur_test_index = 0\n",
    "            yield ret\n",
    "\n",
    "    def load_train_data(self, desc_file='train_corpus.json'):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'train')\n",
    "        self.fit_train()\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('train')\n",
    "\n",
    "    def load_validation_data(self, desc_file='valid_corpus.json'):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'validation')\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('valid')\n",
    "\n",
    "    def load_test_data(self, desc_file='test_corpus.json'):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'test')\n",
    "    \n",
    "    def load_metadata_from_desc_file(self, desc_file, partition):\n",
    "        \"\"\" Read metadata from a JSON-line file\n",
    "            (possibly takes long, depending on the filesize)\n",
    "        Params:\n",
    "            desc_file (str):  Path to a JSON-line file that contains labels and\n",
    "                paths to the audio files\n",
    "            partition (str): One of 'train', 'validation' or 'test'\n",
    "        \"\"\"\n",
    "        audio_paths, durations, texts = [], [], []\n",
    "        with open(desc_file) as json_line_file:\n",
    "            for line_num, json_line in enumerate(json_line_file):\n",
    "                try:\n",
    "                    spec = json.loads(json_line)\n",
    "                    if float(spec['duration']) > self.max_duration:\n",
    "                        continue\n",
    "                    audio_paths.append(spec['key'])\n",
    "                    durations.append(float(spec['duration']))\n",
    "                    texts.append(spec['text'])\n",
    "                except Exception as e:\n",
    "                    # Change to (KeyError, ValueError) or\n",
    "                    # (KeyError,json.decoder.JSONDecodeError), depending on\n",
    "                    # json module version\n",
    "                    print('Error reading line #{}: {}'\n",
    "                                .format(line_num, json_line))\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths = audio_paths\n",
    "            self.train_durations = durations\n",
    "            self.train_texts = texts\n",
    "        elif partition == 'validation':\n",
    "            self.valid_audio_paths = audio_paths\n",
    "            self.valid_durations = durations\n",
    "            self.valid_texts = texts\n",
    "        elif partition == 'test':\n",
    "            self.test_audio_paths = audio_paths\n",
    "            self.test_durations = durations\n",
    "            self.test_texts = texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition to load metadata. \"\n",
    "             \"Must be train/validation/test\")\n",
    "            \n",
    "    def fit_train(self, k_samples=100):\n",
    "        \"\"\" Estimate the mean and std of the features from the training set\n",
    "        Params:\n",
    "            k_samples (int): Use this number of samples for estimation\n",
    "        \"\"\"\n",
    "        k_samples = min(k_samples, len(self.train_audio_paths))\n",
    "        samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
    "        feats = [self.featurize(s) for s in samples]\n",
    "        feats = np.vstack(feats)\n",
    "        self.feats_mean = np.mean(feats, axis=0)\n",
    "        self.feats_std = np.std(feats, axis=0)\n",
    "        \n",
    "    def featurize(self, audio_clip):\n",
    "        \"\"\" For a given audio clip, calculate the corresponding feature\n",
    "        Params:\n",
    "            audio_clip (str): Path to the audio clip\n",
    "        \"\"\"\n",
    "        if self.spectrogram:\n",
    "            return spectrogram_from_file(\n",
    "                audio_clip, step=self.step, window=self.window,\n",
    "                max_freq=self.max_freq)\n",
    "        else:\n",
    "            (rate, sig) = wav.read(audio_clip)\n",
    "            return mfcc(sig, rate, numcep=self.mfcc_dim)\n",
    "\n",
    "    def normalize(self, feature, eps=1e-14):\n",
    "        \"\"\" Center a feature using the mean and std\n",
    "        Params:\n",
    "            feature (numpy.ndarray): Feature to normalize\n",
    "        \"\"\"\n",
    "        return (feature - self.feats_mean) / (self.feats_std + eps)\n",
    "\n",
    "def shuffle_data(audio_paths, durations, texts):\n",
    "    \"\"\" Shuffle the data (called after making a complete pass through \n",
    "        training or validation data during the training process)\n",
    "    Params:\n",
    "        audio_paths (list): Paths to audio clips\n",
    "        durations (list): Durations of utterances for each audio clip\n",
    "        texts (list): Sentences uttered in each audio clip\n",
    "    \"\"\"\n",
    "    p = np.random.permutation(len(audio_paths))\n",
    "    audio_paths = [audio_paths[i] for i in p] \n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "def sort_data(audio_paths, durations, texts):\n",
    "    \"\"\" Sort the data by duration \n",
    "    Params:\n",
    "        audio_paths (list): Paths to audio clips\n",
    "        durations (list): Durations of utterances for each audio clip\n",
    "        texts (list): Sentences uttered in each audio clip\n",
    "    \"\"\"\n",
    "    p = np.argsort(durations).tolist()\n",
    "    audio_paths = [audio_paths[i] for i in p]\n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "def vis_train_features(index=0):\n",
    "    \"\"\" Visualizing the data point in the training set at the supplied index\n",
    "    \"\"\"\n",
    "    # obtain spectrogram\n",
    "    audio_gen = AudioGenerator(spectrogram=True)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_audio_path = audio_gen.train_audio_paths[index]\n",
    "    vis_spectrogram_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # obtain mfcc\n",
    "    audio_gen = AudioGenerator(spectrogram=False)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_mfcc_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # obtain text label\n",
    "    vis_text = audio_gen.train_texts[index]\n",
    "    # obtain raw audio\n",
    "    vis_raw_audio, _ = librosa.load(vis_audio_path)\n",
    "    # print total number of training examples\n",
    "    print('There are %d total training examples.' % len(audio_gen.train_audio_paths))\n",
    "    # return labels for plotting\n",
    "    return vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path\n",
    "\n",
    "\n",
    "def plot_raw_audio(vis_raw_audio):\n",
    "    # plot the raw audio signal\n",
    "    fig = plt.figure(figsize=(12,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    steps = len(vis_raw_audio)\n",
    "    ax.plot(np.linspace(1, steps, steps), vis_raw_audio)\n",
    "    plt.title('Audio Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "def plot_mfcc_feature(vis_mfcc_feature):\n",
    "    # plot the MFCC feature\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized MFCC')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('MFCC Coefficient')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectrogram_feature(vis_spectrogram_feature):\n",
    "    # plot the normalized spectrogram\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_spectrogram_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized Spectrogram')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('Frequency')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0318dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_dim, filters, kernel_size, conv_stride, conv_border_mode, units, recur_layers=1, output_dim=29):\n",
    "    \"\"\" Build a deep network for speech \n",
    "    \"\"\"  \n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # TODO: Specify the layers in your network\n",
    "    # Add convolutional layer\n",
    "    conv_1d = Conv1D(filters, kernel_size,\n",
    "                     strides=conv_stride,\n",
    "                     padding=conv_border_mode,\n",
    "                     activation='relu',\n",
    "                     name='conv1d')(input_data)\n",
    "    # Add batch normalization\n",
    "    bn_cnn = BatchNormalization(name='bn_conv_1d')(conv_1d)\n",
    "    bn_rnn = bn_cnn \n",
    "    \n",
    "    for i in range(recur_layers):\n",
    "        num = i + 1\n",
    "        # Add bidirecitonal layer\n",
    "        layer_name = \"rnn\"+str(i+1)\n",
    "        bidir_rnn = Bidirectional(GRU(units, return_sequences=True, reset_after=False, \n",
    "                                      implementation=2, dropout=0.2,\n",
    "                                      name=layer_name),\n",
    "                                  merge_mode='concat')(bn_rnn)\n",
    "        # Add batch normalization\n",
    "        batch_name = \"bn\"+layer_name\n",
    "        bn_rnn = BatchNormalization(name = batch_name)(bidir_rnn)\n",
    "\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # TODO: Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    # TODO: Specify model.output_length\n",
    "    model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, conv_border_mode, conv_stride)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d7eda2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, None, 161)]       0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 200)         354400    \n",
      "                                                                 \n",
      " bn_conv_1d (BatchNormalizat  (None, None, 200)        800       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, None, 400)        482400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bnrnn1 (BatchNormalization)  (None, None, 400)        1600      \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, None, 400)        722400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bnrnn2 (BatchNormalization)  (None, None, 400)        1600      \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 29)         11629     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, None, 29)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,574,829\n",
      "Trainable params: 1,572,829\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model(input_dim=161,\n",
    "              filters=200,\n",
    "              kernel_size=11,\n",
    "              conv_stride=2,\n",
    "              conv_border_mode='valid',\n",
    "              units=200,\n",
    "              recur_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e84d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_to_softmax, model_path):\n",
    "    data_gen = AudioGenerator()\n",
    "    audio_path = \"../SpeechRecognition-V2/FinalTestFiles/Trump New.wav\"\n",
    "    data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    \n",
    "    # obtain and decode the acoustic model's predictions\n",
    "    input_to_softmax.load_weights(model_path)\n",
    "    prediction = input_to_softmax.predict(np.expand_dims(data_point, axis=0))\n",
    "    output_length = [input_to_softmax.output_length(data_point.shape[0])] \n",
    "    pred_ints = (K.eval(K.ctc_decode(prediction, output_length)[0][0])+1).flatten().tolist()\n",
    "    \n",
    "    # play the audio file, and display the true and predicted transcriptions\n",
    "    print('-'*80)\n",
    "    print(model_path)\n",
    "    print(audio_path)\n",
    "    print('-'*80)\n",
    "    print(pred_ints)\n",
    "    print('-'*80)\n",
    "    print('Predicted transcription:\\n' + '\\n' + ''.join(int_sequence_to_text(pred_ints)))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9f991bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "GRU(reset_after=False) is not compatible with GRU(reset_after=True)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/model_end.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(input_to_softmax, model_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m data_point \u001b[38;5;241m=\u001b[39m data_gen\u001b[38;5;241m.\u001b[39mnormalize(data_gen\u001b[38;5;241m.\u001b[39mfeaturize(audio_path))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# obtain and decode the acoustic model's predictions\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43minput_to_softmax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m prediction \u001b[38;5;241m=\u001b[39m input_to_softmax\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mexpand_dims(data_point, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      9\u001b[0m output_length \u001b[38;5;241m=\u001b[39m [input_to_softmax\u001b[38;5;241m.\u001b[39moutput_length(data_point\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])] \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/saving/hdf5_format.py:581\u001b[0m, in \u001b[0;36m_convert_rnn_weights\u001b[0;34m(layer, weights)\u001b[0m\n\u001b[1;32m    579\u001b[0m types \u001b[38;5;241m=\u001b[39m (source, target)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRU(reset_after=False)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[0;32m--> 581\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not compatible with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m types)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCuDNNGRU\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    583\u001b[0m   weights \u001b[38;5;241m=\u001b[39m convert_gru_weights(weights, from_cudnn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: GRU(reset_after=False) is not compatible with GRU(reset_after=True)"
     ]
    }
   ],
   "source": [
    "predict(model, \"models/model_end.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28386978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
